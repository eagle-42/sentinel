"""
üíæ Stockage Unifi√© Sentinel2
Gestion centralis√©e du stockage des donn√©es
"""

import json
import pickle
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List, Optional, Union

import numpy as np
import pandas as pd
import torch
from loguru import logger

from ..constants import CONSTANTS


class ParquetStorage:
    """Stockage en format Parquet optimis√©"""
    
    def __init__(self, base_path: Path = None):
        """Initialise le stockage Parquet"""
        self.base_path = base_path or CONSTANTS.DATA_ROOT
        self.base_path.mkdir(parents=True, exist_ok=True)
        
        logger.info(f"üíæ Stockage Parquet initialis√©: {self.base_path}")
    
    def save_prices(self, 
                   data: pd.DataFrame, 
                   ticker: str, 
                   interval: str = "1min") -> Path:
        """Sauvegarde les donn√©es de prix de mani√®re incr√©mentale"""
        try:
            # Chemin de sauvegarde
            file_path = CONSTANTS.get_data_path("prices", ticker, interval)
            file_path.parent.mkdir(parents=True, exist_ok=True)
            
            # Sauvegarde incr√©mentale : charger les donn√©es existantes et ajouter les nouvelles
            if file_path.exists():
                try:
                    existing_data = pd.read_parquet(file_path)
                    # Concat√©ner et d√©dupliquer par timestamp
                    combined_data = pd.concat([existing_data, data], ignore_index=True)
                    combined_data = combined_data.drop_duplicates(subset=['ts_utc'], keep='last')
                    combined_data = combined_data.sort_values('ts_utc')
                    data_to_save = combined_data
                    logger.info(f"üíæ Prix {ticker} mis √† jour incr√©mentalement: {len(existing_data)} ‚Üí {len(combined_data)} lignes")
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è Erreur lecture fichier existant {ticker}, sauvegarde compl√®te: {e}")
                    data_to_save = data
            else:
                data_to_save = data
                logger.info(f"üíæ Nouveau fichier prix {ticker} cr√©√©: {len(data)} lignes")
            
            # Sauvegarder
            data_to_save.to_parquet(file_path, index=False, compression='snappy')
            
            logger.info(f"üíæ Prix {ticker} sauvegard√©: {file_path}")
            return file_path
            
        except Exception as e:
            logger.error(f"‚ùå Erreur sauvegarde prix {ticker}: {e}")
            raise
    
    def load_prices(self, 
                   ticker: str, 
                   interval: str = "1min") -> pd.DataFrame:
        """Charge les donn√©es de prix"""
        try:
            file_path = CONSTANTS.get_data_path("prices", ticker, interval)
            
            if not file_path.exists():
                logger.warning(f"‚ö†Ô∏è Fichier prix non trouv√©: {file_path}")
                return pd.DataFrame()
            
            df = pd.read_parquet(file_path)
            logger.info(f"üíæ Prix {ticker} charg√©: {len(df)} lignes")
            return df
            
        except Exception as e:
            logger.error(f"‚ùå Erreur chargement prix {ticker}: {e}")
            return pd.DataFrame()
    
    def save_news(self, 
                 data: pd.DataFrame, 
                 ticker: str = None) -> Path:
        """Sauvegarde les donn√©es de news de mani√®re incr√©mentale"""
        try:
            # Chemin de sauvegarde
            if ticker:
                file_path = CONSTANTS.get_data_path("news", ticker)
            else:
                file_path = CONSTANTS.NEWS_DIR / "all_news.parquet"  # Un seul fichier pour toutes les news
            
            file_path.parent.mkdir(parents=True, exist_ok=True)
            
            # Sauvegarde incr√©mentale : charger les donn√©es existantes et ajouter les nouvelles
            if file_path.exists():
                try:
                    existing_data = pd.read_parquet(file_path)
                    # Concat√©ner et d√©dupliquer par timestamp et title
                    combined_data = pd.concat([existing_data, data], ignore_index=True)
                    combined_data = combined_data.drop_duplicates(subset=['timestamp', 'title'], keep='last')
                    combined_data = combined_data.sort_values('timestamp')
                    data_to_save = combined_data
                    logger.info(f"üíæ News mis √† jour incr√©mentalement: {len(existing_data)} ‚Üí {len(combined_data)} lignes")
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è Erreur lecture fichier news existant, sauvegarde compl√®te: {e}")
                    data_to_save = data
            else:
                data_to_save = data
                logger.info(f"üíæ Nouveau fichier news cr√©√©: {len(data)} lignes")
            
            # Sauvegarder
            data_to_save.to_parquet(file_path, index=False, compression='snappy')
            
            logger.info(f"üíæ News sauvegard√©: {file_path}")
            return file_path
            
        except Exception as e:
            logger.error(f"‚ùå Erreur sauvegarde news: {e}")
            raise
    
    def load_news(self, 
                 ticker: str = None,
                 latest: bool = True) -> pd.DataFrame:
        """Charge les donn√©es de news"""
        try:
            if ticker:
                file_path = CONSTANTS.get_data_path("news", ticker)
            else:
                # Charger le fichier le plus r√©cent
                news_files = list(CONSTANTS.NEWS_DIR.glob("*.parquet"))
                if not news_files:
                    logger.warning("‚ö†Ô∏è Aucun fichier news trouv√©")
                    return pd.DataFrame()
                
                file_path = max(news_files, key=lambda x: x.stat().st_mtime)
            
            if not file_path.exists():
                logger.warning(f"‚ö†Ô∏è Fichier news non trouv√©: {file_path}")
                return pd.DataFrame()
            
            df = pd.read_parquet(file_path)
            logger.info(f"üíæ News charg√©: {len(df)} lignes")
            return df
            
        except Exception as e:
            logger.error(f"‚ùå Erreur chargement news: {e}")
            return pd.DataFrame()
    
    def save_sentiment(self, 
                      data: pd.DataFrame, 
                      ticker: str) -> Path:
        """Sauvegarde les donn√©es de sentiment de mani√®re incr√©mentale"""
        try:
            file_path = CONSTANTS.get_data_path("sentiment", ticker)
            file_path.parent.mkdir(parents=True, exist_ok=True)
            
            # Sauvegarde incr√©mentale : charger les donn√©es existantes et ajouter les nouvelles
            if file_path.exists():
                try:
                    existing_data = pd.read_parquet(file_path)
                    # Concat√©ner et d√©dupliquer par timestamp
                    combined_data = pd.concat([existing_data, data], ignore_index=True)
                    combined_data = combined_data.drop_duplicates(subset=['timestamp'], keep='last')
                    combined_data = combined_data.sort_values('timestamp')
                    data_to_save = combined_data
                    logger.info(f"üíæ Sentiment {ticker} mis √† jour incr√©mentalement: {len(existing_data)} ‚Üí {len(combined_data)} lignes")
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è Erreur lecture fichier sentiment existant {ticker}, sauvegarde compl√®te: {e}")
                    data_to_save = data
            else:
                data_to_save = data
                logger.info(f"üíæ Nouveau fichier sentiment {ticker} cr√©√©: {len(data)} lignes")
            
            # Sauvegarder
            data_to_save.to_parquet(file_path, index=False, compression='snappy')
            
            logger.info(f"üíæ Sentiment {ticker} sauvegard√©: {file_path}")
            return file_path
            
        except Exception as e:
            logger.error(f"‚ùå Erreur sauvegarde sentiment {ticker}: {e}")
            raise
    
    def load_sentiment(self, ticker: str) -> pd.DataFrame:
        """Charge les donn√©es de sentiment"""
        try:
            file_path = CONSTANTS.get_data_path("sentiment", ticker)
            
            if not file_path.exists():
                logger.warning(f"‚ö†Ô∏è Fichier sentiment non trouv√©: {file_path}")
                return pd.DataFrame()
            
            df = pd.read_parquet(file_path)
            logger.info(f"üíæ Sentiment {ticker} charg√©: {len(df)} lignes")
            return df
            
        except Exception as e:
            logger.error(f"‚ùå Erreur chargement sentiment {ticker}: {e}")
            return pd.DataFrame()

class DataStorage:
    """Stockage unifi√© pour toutes les donn√©es Sentinel2"""
    
    def __init__(self):
        """Initialise le stockage unifi√©"""
        self.parquet = ParquetStorage()
        
        logger.info("üíæ Stockage unifi√© initialis√©")
    
    def save_crawl_results(self, 
                          results: Dict[str, Any],
                          timestamp: datetime = None) -> Dict[str, Path]:
        """Sauvegarde les r√©sultats d'un crawling complet"""
        if timestamp is None:
            timestamp = datetime.now()
        
        saved_files = {}
        
        try:
            # Sauvegarder les prix
            for ticker, df in results.get("prices", {}).items():
                if not df.empty:
                    file_path = self.parquet.save_prices(df, ticker)
                    saved_files[f"prices_{ticker}"] = file_path
            
            # Sauvegarder les news
            news = results.get("news", [])
            if news:
                news_df = pd.DataFrame(news)
                file_path = self.parquet.save_news(news_df)
                saved_files["news"] = file_path
            
            # Sauvegarder les m√©tadonn√©es
            metadata = {
                "timestamp": timestamp.isoformat(),
                "total_tickers": len(results.get("prices", {})),
                "total_news": len(results.get("news", [])),
                "crawl_time": results.get("crawl_time", 0),
                "saved_files": {k: str(v) for k, v in saved_files.items()}
            }
            
            metadata_path = CONSTANTS.DATA_ROOT / f"crawl_metadata_{timestamp.strftime('%Y%m%d_%H%M%S')}.json"
            with open(metadata_path, 'w') as f:
                json.dump(metadata, f, indent=2)
            
            saved_files["metadata"] = metadata_path
            
            logger.info(f"üíæ Crawling sauvegard√©: {len(saved_files)} fichiers")
            return saved_files
            
        except Exception as e:
            logger.error(f"‚ùå Erreur sauvegarde crawling: {e}")
            raise
    
    def load_latest_data(self, 
                        tickers: List[str] = None,
                        include_news: bool = True) -> Dict[str, Any]:
        """Charge les donn√©es les plus r√©centes"""
        tickers = tickers or CONSTANTS.TICKERS
        data = {}
        
        try:
            # Charger les prix
            for ticker in tickers:
                df = self.parquet.load_prices(ticker)
                if not df.empty:
                    data[f"prices_{ticker}"] = df
            
            # Charger les news
            if include_news:
                news_df = self.parquet.load_news()
                if not news_df.empty:
                    data["news"] = news_df
            
            logger.info(f"üíæ Donn√©es charg√©es: {len(data)} datasets")
            return data
            
        except Exception as e:
            logger.error(f"‚ùå Erreur chargement donn√©es: {e}")
            return {}
    
    def save_model_artifacts(self, 
                           ticker: str,
                           model: Any,
                           scaler: Any,
                           metrics: Dict[str, float],
                           version: int = None) -> Dict[str, Path]:
        """Sauvegarde les artefacts d'un mod√®le"""
        try:
            # Chemin du mod√®le
            if version:
                model_dir = CONSTANTS.get_model_path(ticker, version)
            else:
                # Trouver la prochaine version
                base_dir = CONSTANTS.get_model_path(ticker)
                existing_versions = [d for d in base_dir.iterdir() if d.is_dir() and d.name.startswith("version")]
                version = max([int(d.name.replace("version", "")) for d in existing_versions], default=0) + 1
                model_dir = base_dir / f"version{version}"
            
            model_dir.mkdir(parents=True, exist_ok=True)
            
            saved_files = {}
            
            # Sauvegarder le mod√®le
            model_path = model_dir / "lstm_model.pth"
            torch.save({
                'model_state_dict': model.state_dict(),
                'model_config': {
                    'input_size': len(CONSTANTS.get_feature_columns()),
                    'hidden_sizes': CONSTANTS.LSTM_HIDDEN_SIZES,
                    'dropout_rate': CONSTANTS.LSTM_DROPOUT_RATE,
                    'task': 'regression'
                }
            }, model_path)
            saved_files["model"] = model_path
            
            # Sauvegarder le scaler
            scaler_path = model_dir / "scaler.pkl"
            with open(scaler_path, 'wb') as f:
                pickle.dump(scaler, f)
            saved_files["scaler"] = scaler_path
            
            # Sauvegarder les m√©triques
            metrics_path = model_dir / "metrics.json"
            with open(metrics_path, 'w') as f:
                json.dump(metrics, f, indent=2)
            saved_files["metrics"] = metrics_path
            
            # Sauvegarder l'historique d'entra√Ænement
            history_path = model_dir / "training_history.json"
            with open(history_path, 'w') as f:
                json.dump({}, f)  # √Ä remplir par le processus d'entra√Ænement
            saved_files["history"] = history_path
            
            logger.info(f"üíæ Mod√®le {ticker} v{version} sauvegard√©")
            return saved_files
            
        except Exception as e:
            logger.error(f"‚ùå Erreur sauvegarde mod√®le {ticker}: {e}")
            raise
    
    def get_data_summary(self) -> Dict[str, Any]:
        """Retourne un r√©sum√© des donn√©es disponibles"""
        summary = {
            "prices": {},
            "news": {},
            "sentiment": {},
            "models": {}
        }
        
        try:
            # R√©sum√© des prix
            for ticker in CONSTANTS.TICKERS:
                df = self.parquet.load_prices(ticker)
                if not df.empty:
                    summary["prices"][ticker] = {
                        "rows": len(df),
                        "date_range": f"{df['ts_utc'].min()} to {df['ts_utc'].max()}" if 'ts_utc' in df.columns else "N/A"
                    }
            
            # R√©sum√© des news
            news_df = self.parquet.load_news()
            if not news_df.empty:
                summary["news"] = {
                    "rows": len(news_df),
                    "sources": news_df['source'].nunique() if 'source' in news_df.columns else 0
                }
            
            # R√©sum√© des mod√®les
            for ticker in CONSTANTS.TICKERS:
                model_dir = CONSTANTS.get_model_path(ticker)
                if model_dir.exists():
                    versions = [d for d in model_dir.iterdir() if d.is_dir() and d.name.startswith("version")]
                    summary["models"][ticker] = {
                        "versions": len(versions),
                        "latest": max([int(d.name.replace("version", "")) for d in versions], default=0) if versions else 0
                    }
            
            return summary
            
        except Exception as e:
            logger.error(f"‚ùå Erreur r√©sum√© donn√©es: {e}")
            return summary
