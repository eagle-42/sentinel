"""
üï∑Ô∏è Crawling Temps R√©el Sentinel2
Collecte de donn√©es prix et news en temps r√©el
"""

import os
import time
from datetime import datetime, timedelta
from typing import Any, Dict, List, Optional, Union

import feedparser
import numpy as np
import pandas as pd
import requests
import yfinance as yf
from loguru import logger

from ..config import config
from ..constants import CONSTANTS


class PriceCrawler:
    """Crawler pour les donn√©es de prix"""
    
    def __init__(self):
        """Initialise le crawler de prix"""
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        })
        
        logger.info("üí∞ Crawler de prix initialis√©")
    
    def get_yahoo_prices(self, 
                        ticker: str, 
                        interval: str = None, 
                        period: str = None) -> pd.DataFrame:
        """R√©cup√®re les prix depuis Yahoo Finance"""
        try:
            interval = interval or CONSTANTS.PRICE_INTERVAL
            period = period or CONSTANTS.PRICE_PERIOD
            
            logger.info(f"üí∞ R√©cup√©ration prix {ticker} - {interval}/{period}")
            
            # T√©l√©charger les donn√©es
            stock = yf.Ticker(ticker)
            df = stock.history(interval=interval, period=period)
            
            if df.empty:
                logger.warning(f"‚ö†Ô∏è Aucune donn√©e pour {ticker}")
                return pd.DataFrame()
            
            # Normaliser les colonnes
            df = df.reset_index()
            df.columns = df.columns.str.lower()
            
            # Ajouter timestamp UTC
            if 'datetime' in df.columns:
                df['ts_utc'] = pd.to_datetime(df['datetime']).dt.tz_convert('UTC')
            else:
                df['ts_utc'] = pd.Timestamp.now(tz='UTC')
            
            # Ajouter ticker
            df['ticker'] = ticker.upper()
            
            # V√©rifier les colonnes requises
            required_cols = ['open', 'high', 'low', 'close', 'volume']
            missing_cols = [col for col in required_cols if col not in df.columns]
            if missing_cols:
                logger.error(f"‚ùå Colonnes manquantes: {missing_cols}")
                return pd.DataFrame()
            
            logger.info(f"‚úÖ {len(df)} lignes r√©cup√©r√©es pour {ticker}")
            return df
            
        except Exception as e:
            logger.error(f"‚ùå Erreur r√©cup√©ration prix {ticker}: {e}")
            return pd.DataFrame()
    
    def get_polygon_prices(self, 
                          ticker: str, 
                          api_key: str = None,
                          interval: str = "1min",
                          days: int = 1) -> pd.DataFrame:
        """R√©cup√®re les prix depuis Polygon API"""
        try:
            api_key = api_key or os.getenv("POLYGON_API_KEY")
            if not api_key:
                logger.warning("‚ö†Ô∏è Cl√© API Polygon non fournie")
                return pd.DataFrame()
            
            # Calculer les dates
            end_date = datetime.now()
            start_date = end_date - timedelta(days=days)
            
            # URL de l'API
            url = f"https://api.polygon.io/v2/aggs/ticker/{ticker}/range/1/{interval}/{start_date.strftime('%Y-%m-%d')}/{end_date.strftime('%Y-%m-%d')}"
            
            params = {
                "apikey": api_key,
                "adjusted": "true",
                "sort": "asc"
            }
            
            logger.info(f"üí∞ R√©cup√©ration Polygon {ticker} - {interval}")
            
            response = self.session.get(url, params=params, timeout=30)
            response.raise_for_status()
            
            data = response.json()
            
            if data.get("status") != "OK" or not data.get("results"):
                logger.warning(f"‚ö†Ô∏è Aucune donn√©e Polygon pour {ticker}")
                return pd.DataFrame()
            
            # Convertir en DataFrame
            results = data["results"]
            df_data = []
            
            for item in results:
                df_data.append({
                    'ts_utc': pd.to_datetime(item['t'], unit='ms', utc=True),
                    'open': item['o'],
                    'high': item['h'],
                    'low': item['l'],
                    'close': item['c'],
                    'volume': item['v'],
                    'ticker': ticker.upper()
                })
            
            df = pd.DataFrame(df_data)
            logger.info(f"‚úÖ {len(df)} lignes Polygon r√©cup√©r√©es pour {ticker}")
            return df
            
        except Exception as e:
            logger.error(f"‚ùå Erreur Polygon {ticker}: {e}")
            return pd.DataFrame()

class NewsCrawler:
    """Crawler pour les donn√©es de news"""
    
    def __init__(self):
        """Initialise le crawler de news"""
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        })
        
        logger.info("üì∞ Crawler de news initialis√©")
    
    def get_rss_news(self, urls: List[str], max_items: int = 100) -> List[Dict[str, Any]]:
        """R√©cup√®re les news depuis les feeds RSS"""
        all_news = []
        
        for url in urls:
            try:
                logger.info(f"üì∞ R√©cup√©ration RSS: {url}")
                
                # Parser le feed RSS
                feed = feedparser.parse(url)
                
                if feed.bozo:
                    logger.warning(f"‚ö†Ô∏è Feed RSS malform√©: {url}")
                    continue
                
                # Extraire les articles
                for entry in feed.entries[:max_items]:
                    news_item = {
                        'title': entry.get('title', ''),
                        'summary': entry.get('summary', ''),
                        'link': entry.get('link', ''),
                        'published': entry.get('published', ''),
                        'source': url,
                        'ticker': self._detect_ticker(entry.get('title', '') + ' ' + entry.get('summary', ''))
                    }
                    all_news.append(news_item)
                
                logger.info(f"‚úÖ {len(feed.entries)} articles RSS r√©cup√©r√©s de {url}")
                
            except Exception as e:
                logger.error(f"‚ùå Erreur RSS {url}: {e}")
                continue
        
        logger.info(f"üì∞ Total: {len(all_news)} articles RSS r√©cup√©r√©s")
        return all_news
    
    def get_newsapi_news(self, 
                        api_key: str = None,
                        sources: str = None,
                        language: str = "en",
                        country: str = "us",
                        max_items: int = 100) -> List[Dict[str, Any]]:
        """R√©cup√®re les news depuis NewsAPI"""
        try:
            api_key = api_key or os.getenv("NEWSAPI_KEY")
            if not api_key:
                logger.warning("‚ö†Ô∏è Cl√© API NewsAPI non fournie")
                return []
            
            sources = sources or os.getenv("NEWSAPI_SOURCES", "reuters,bloomberg,associated-press,cnbc")
            
            url = "https://newsapi.org/v2/top-headlines"
            params = {
                "apiKey": api_key,
                "sources": sources,
                "language": language,
                "country": country,
                "pageSize": min(max_items, 100)
            }
            
            logger.info(f"üì∞ R√©cup√©ration NewsAPI: {sources}")
            
            response = self.session.get(url, params=params, timeout=30)
            response.raise_for_status()
            
            data = response.json()
            
            if data.get("status") != "ok":
                logger.error(f"‚ùå Erreur NewsAPI: {data.get('message', 'Unknown error')}")
                return []
            
            articles = data.get("articles", [])
            news_items = []
            
            for article in articles:
                news_item = {
                    'title': article.get('title', ''),
                    'summary': article.get('description', ''),
                    'body': article.get('content', ''),
                    'link': article.get('url', ''),
                    'published': article.get('publishedAt', ''),
                    'source': article.get('source', {}).get('name', 'NewsAPI'),
                    'ticker': self._detect_ticker(article.get('title', '') + ' ' + article.get('description', ''))
                }
                news_items.append(news_item)
            
            logger.info(f"‚úÖ {len(news_items)} articles NewsAPI r√©cup√©r√©s")
            return news_items
            
        except Exception as e:
            logger.error(f"‚ùå Erreur NewsAPI: {e}")
            return []
    
    def _detect_ticker(self, text: str) -> Optional[str]:
        """D√©tecte le ticker dans un texte"""
        text_upper = text.upper()
        
        # Mots-cl√©s pour chaque ticker
        ticker_keywords = {
            "SPY": ["SPY", "S&P 500", "SPDR", "SPDR S&P 500", "ETF", "index"],
            "NVDA": ["NVDA", "NVIDIA", "Nvidia", "nvidia", "GPU", "AI", "artificial intelligence"]
        }
        
        for ticker, keywords in ticker_keywords.items():
            if any(keyword in text_upper for keyword in keywords):
                return ticker
        
        return None

class DataCrawler:
    """Crawler principal pour toutes les donn√©es"""
    
    def __init__(self):
        """Initialise le crawler principal"""
        self.price_crawler = PriceCrawler()
        self.news_crawler = NewsCrawler()
        
        logger.info("üï∑Ô∏è Crawler principal initialis√©")
    
    def crawl_prices(self, 
                    tickers: List[str] = None,
                    interval: str = None,
                    period: str = None) -> Dict[str, pd.DataFrame]:
        """Crawl les prix pour tous les tickers"""
        tickers = tickers or CONSTANTS.TICKERS
        results = {}
        
        for ticker in tickers:
            logger.info(f"üí∞ Crawling prix {ticker}")
            
            # Essayer Yahoo Finance d'abord
            df = self.price_crawler.get_yahoo_prices(ticker, interval, period)
            
            if df.empty:
                # Essayer Polygon en fallback
                df = self.price_crawler.get_polygon_prices(ticker)
            
            if not df.empty:
                results[ticker] = df
                logger.info(f"‚úÖ Prix {ticker}: {len(df)} lignes")
            else:
                logger.warning(f"‚ö†Ô∏è Aucun prix r√©cup√©r√© pour {ticker}")
        
        return results
    
    def crawl_news(self, 
                  rss_urls: List[str] = None,
                  newsapi_enabled: bool = None,
                  max_items: int = 100) -> List[Dict[str, Any]]:
        """Crawl les news depuis toutes les sources"""
        all_news = []
        
        rss_urls = rss_urls or CONSTANTS.NEWS_FEEDS
        newsapi_enabled = newsapi_enabled if newsapi_enabled is not None else config.get("news.newsapi_enabled", False)
        
        # RSS Feeds
        if rss_urls:
            logger.info("üì∞ Crawling RSS feeds")
            rss_news = self.news_crawler.get_rss_news(rss_urls, max_items)
            all_news.extend(rss_news)
        
        # NewsAPI
        if newsapi_enabled:
            logger.info("üì∞ Crawling NewsAPI")
            newsapi_news = self.news_crawler.get_newsapi_news(max_items=max_items)
            all_news.extend(newsapi_news)
        
        # Filtrer les doublons bas√©s sur le titre
        seen_titles = set()
        unique_news = []
        
        for news in all_news:
            title = news.get('title', '').lower().strip()
            if title and title not in seen_titles:
                seen_titles.add(title)
                unique_news.append(news)
        
        logger.info(f"üì∞ Total news uniques: {len(unique_news)}")
        return unique_news
    
    def crawl_all(self, 
                 tickers: List[str] = None,
                 price_interval: str = None,
                 price_period: str = None,
                 max_news_items: int = 100) -> Dict[str, Any]:
        """Crawl toutes les donn√©es (prix + news)"""
        logger.info("üï∑Ô∏è D√©but du crawling complet")
        start_time = time.time()
        
        # Crawler les prix
        prices = self.crawl_prices(tickers, price_interval, price_period)
        
        # Crawler les news
        news = self.crawl_news(max_items=max_news_items)
        
        # R√©sultats
        results = {
            "prices": prices,
            "news": news,
            "crawl_time": time.time() - start_time,
            "total_tickers": len(prices),
            "total_news": len(news)
        }
        
        logger.info(f"‚úÖ Crawling termin√© en {results['crawl_time']:.2f}s")
        return results
