"""
Script de consolidation des donnÃ©es Parquet
Consolide tous les fichiers par date en fichiers unifiÃ©s
"""

import sys
from datetime import datetime
from pathlib import Path

import pandas as pd

# Ajouter le chemin src pour les imports
sys.path.insert(0, str(Path(__file__).parent.parent / "src"))

from loguru import logger

from constants import CONSTANTS


def consolidate_news_data():
    """Consolide tous les fichiers de news en un seul fichier unifiÃ©"""
    logger.info("ğŸ“° Consolidation des donnÃ©es de news...")

    news_dir = CONSTANTS.NEWS_DIR
    all_news_file = news_dir / "all_news.parquet"

    # Chercher tous les fichiers de news par date
    news_files = list(news_dir.glob("news_*.parquet"))

    if not news_files:
        logger.info("â„¹ï¸ Aucun fichier de news Ã  consolider")
        return

    # Charger et fusionner tous les fichiers
    all_data = []
    for news_file in news_files:
        try:
            df = pd.read_parquet(news_file)
            all_data.append(df)
            logger.info(f"ğŸ“„ ChargÃ©: {news_file.name} ({len(df)} lignes)")
        except Exception as e:
            logger.error(f"âŒ Erreur lecture {news_file.name}: {e}")

    if not all_data:
        logger.warning("âš ï¸ Aucune donnÃ©e de news valide")
        return

    # Fusionner toutes les donnÃ©es
    consolidated_df = pd.concat(all_data, ignore_index=True)

    # Supprimer les doublons
    original_count = len(consolidated_df)
    consolidated_df = consolidated_df.drop_duplicates(subset=["title", "source", "timestamp"], keep="last")
    final_count = len(consolidated_df)

    logger.info(
        f"ğŸ“Š Consolidation: {original_count} â†’ {final_count} lignes (supprimÃ© {original_count - final_count} doublons)"
    )

    # Sauvegarder le fichier consolidÃ©
    consolidated_df.to_parquet(all_news_file, index=False)
    logger.success(f"âœ… News consolidÃ©es: {all_news_file} ({final_count} lignes)")

    # Supprimer les anciens fichiers
    for news_file in news_files:
        try:
            news_file.unlink()
            logger.info(f"ğŸ—‘ï¸ SupprimÃ©: {news_file.name}")
        except Exception as e:
            logger.error(f"âŒ Erreur suppression {news_file.name}: {e}")


def consolidate_sentiment_data():
    """Consolide tous les fichiers de sentiment en un seul fichier unifiÃ©"""
    logger.info("ğŸ’­ Consolidation des donnÃ©es de sentiment...")

    sentiment_dir = CONSTANTS.SENTIMENT_DIR
    spy_sentiment_file = sentiment_dir / "spy_sentiment.parquet"

    # Chercher tous les fichiers de sentiment par date
    sentiment_files = list(sentiment_dir.glob("sentiment_*.parquet"))

    if not sentiment_files:
        logger.info("â„¹ï¸ Aucun fichier de sentiment Ã  consolider")
        return

    # Charger et fusionner tous les fichiers
    all_data = []
    for sentiment_file in sentiment_files:
        try:
            df = pd.read_parquet(sentiment_file)
            all_data.append(df)
            logger.info(f"ğŸ“„ ChargÃ©: {sentiment_file.name} ({len(df)} lignes)")
        except Exception as e:
            logger.error(f"âŒ Erreur lecture {sentiment_file.name}: {e}")

    if not all_data:
        logger.warning("âš ï¸ Aucune donnÃ©e de sentiment valide")
        return

    # Fusionner toutes les donnÃ©es
    consolidated_df = pd.concat(all_data, ignore_index=True)

    # Supprimer les doublons
    original_count = len(consolidated_df)
    consolidated_df = consolidated_df.drop_duplicates(subset=["ticker", "ts_utc"], keep="last")
    final_count = len(consolidated_df)

    logger.info(
        f"ğŸ“Š Consolidation: {original_count} â†’ {final_count} lignes (supprimÃ© {original_count - final_count} doublons)"
    )

    # Sauvegarder le fichier consolidÃ©
    consolidated_df.to_parquet(spy_sentiment_file, index=False)
    logger.success(f"âœ… Sentiment consolidÃ©: {spy_sentiment_file} ({final_count} lignes)")

    # Supprimer les anciens fichiers
    for sentiment_file in sentiment_files:
        try:
            sentiment_file.unlink()
            logger.info(f"ğŸ—‘ï¸ SupprimÃ©: {sentiment_file.name}")
        except Exception as e:
            logger.error(f"âŒ Erreur suppression {sentiment_file.name}: {e}")


def main():
    """Fonction principale de consolidation"""
    logger.info("ğŸ”„ === CONSOLIDATION DES DONNÃ‰ES PARQUET ===")

    try:
        # Consolider les news
        consolidate_news_data()

        # Consolider le sentiment
        consolidate_sentiment_data()

        logger.success("âœ… Consolidation terminÃ©e avec succÃ¨s!")

    except Exception as e:
        logger.error(f"âŒ Erreur lors de la consolidation: {e}")
        raise


if __name__ == "__main__":
    main()
