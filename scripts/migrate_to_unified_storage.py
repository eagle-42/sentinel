#!/usr/bin/env python3
"""
üîÑ Migration vers Stockage Unifi√©
Migre les donn√©es existantes vers la nouvelle architecture de stockage
"""

import pandas as pd
import numpy as np
from pathlib import Path
import sys
from datetime import datetime
import shutil

# Ajouter src au path
sys.path.insert(0, str(Path(__file__).parent.parent / "src"))

from src.data.unified_storage import UnifiedDataStorage
from src.constants import CONSTANTS

def migrate_existing_data():
    """Migre toutes les donn√©es existantes vers le stockage unifi√©"""
    print("üîÑ Migration vers stockage unifi√©...")
    
    storage = UnifiedDataStorage()
    
    # 1. Migrer les donn√©es de prix
    migrate_price_data(storage)
    
    # 2. Migrer les donn√©es de news
    migrate_news_data(storage)
    
    # 3. Migrer les donn√©es de features
    migrate_features_data(storage)
    
    # 4. Migrer les mod√®les
    migrate_models(storage)
    
    # 5. Nettoyer les anciens r√©pertoires
    cleanup_old_directories()
    
    print("‚úÖ Migration termin√©e avec succ√®s!")

def migrate_price_data(storage):
    """Migre les donn√©es de prix"""
    print("üìä Migration des donn√©es de prix...")
    
    # Sources de donn√©es existantes
    price_sources = [
        ("data/dataset/1-yfinance/SPY_1999_2025.parquet", "SPY", "1min", "yfinance"),
        ("data/dataset/1-yfinance/NVDA_1999_2025.parquet", "NVDA", "1min", "yfinance"),
        ("data/dataset/2-delta-api-15m/spy_15min_delta.parquet", "SPY", "15min", "delta"),
        ("data/dataset/2-delta-api-15m/nvda_15min_delta.parquet", "NVDA", "15min", "delta"),
        ("data/trading/prices/spy_1min.parquet", "SPY", "1min", "trading"),
        ("data/trading/prices/nvda_1min.parquet", "NVDA", "1min", "trading")
    ]
    
    for source_path, ticker, interval, source in price_sources:
        if Path(source_path).exists():
            try:
                # Charger les donn√©es
                data = pd.read_parquet(source_path)
                
                # Normaliser les colonnes
                data = normalize_price_data(data)
                
                # Sauvegarder dans le stockage unifi√©
                storage.save_price_data(ticker, data, interval, source)
                print(f"  ‚úÖ {ticker} {interval} ({source}) migr√©")
                
            except Exception as e:
                print(f"  ‚ùå Erreur migration {ticker} {interval}: {e}")

def migrate_news_data(storage):
    """Migre les donn√©es de news"""
    print("üì∞ Migration des donn√©es de news...")
    
    # Chercher les fichiers de news existants
    news_dirs = ["data/news", "data/trading/sentiment"]
    
    for news_dir in news_dirs:
        if Path(news_dir).exists():
            news_files = list(Path(news_dir).glob("*.parquet"))
            for news_file in news_files:
                try:
                    # Charger les donn√©es
                    data = pd.read_parquet(news_file)
                    
                    # Normaliser les colonnes
                    data = normalize_news_data(data)
                    
                    # D√©terminer le ticker
                    ticker = extract_ticker_from_filename(news_file.name)
                    
                    # Sauvegarder dans le stockage unifi√©
                    storage.save_news_data(data, ticker, "migrated")
                    print(f"  ‚úÖ News {ticker} migr√© depuis {news_file.name}")
                    
                except Exception as e:
                    print(f"  ‚ùå Erreur migration news {news_file.name}: {e}")

def migrate_features_data(storage):
    """Migre les donn√©es de features"""
    print("üîß Migration des donn√©es de features...")
    
    # Sources de features existantes
    features_sources = [
        ("data/dataset/3-features/spy_features.parquet", "SPY", "technical"),
        ("data/dataset/3-features/nvda_features.parquet", "NVDA", "technical")
    ]
    
    for source_path, ticker, feature_type in features_sources:
        if Path(source_path).exists():
            try:
                # Charger les donn√©es
                data = pd.read_parquet(source_path)
                
                # Normaliser les colonnes
                data = normalize_features_data(data)
                
                # Sauvegarder dans le stockage unifi√©
                storage.save_features_data(ticker, data, feature_type)
                print(f"  ‚úÖ Features {ticker} migr√©")
                
            except Exception as e:
                print(f"  ‚ùå Erreur migration features {ticker}: {e}")

def migrate_models(storage):
    """Migre les mod√®les existants"""
    print("ü§ñ Migration des mod√®les...")
    
    # Mod√®les existants
    model_sources = [
        "data/models/spy",
        "data/models/nvda",
        "data/trading/models/spy",
        "data/trading/models/nvda"
    ]
    
    for model_dir in model_sources:
        if Path(model_dir).exists():
            # Copier les mod√®les vers le nouveau r√©pertoire
            target_dir = storage.models_dir / Path(model_dir).name
            if not target_dir.exists():
                shutil.copytree(model_dir, target_dir)
                print(f"  ‚úÖ Mod√®les {Path(model_dir).name} migr√©")

def normalize_price_data(data):
    """Normalise les donn√©es de prix"""
    # Mapping des colonnes
    column_mapping = {
        'Open': 'open',
        'High': 'high',
        'Low': 'low',
        'Close': 'close',
        'Volume': 'volume',
        'Adj Close': 'adj_close',
        'Date': 'timestamp',
        'Datetime': 'timestamp'
    }
    
    # Renommer les colonnes
    data = data.rename(columns=column_mapping)
    
    # S'assurer que timestamp est datetime
    if 'timestamp' in data.columns:
        data['timestamp'] = pd.to_datetime(data['timestamp'])
    elif 'Date' in data.columns:
        data['timestamp'] = pd.to_datetime(data['Date'])
        data = data.drop('Date', axis=1)
    
    # Ajouter ticker si manquant
    if 'ticker' not in data.columns:
        # Essayer de deviner le ticker depuis le nom du fichier
        data['ticker'] = 'UNKNOWN'
    
    return data

def normalize_news_data(data):
    """Normalise les donn√©es de news"""
    # Mapping des colonnes
    column_mapping = {
        'title': 'title',
        'summary': 'summary',
        'body': 'body',
        'content': 'body',
        'link': 'url',
        'published': 'published_at',
        'publishedAt': 'published_at',
        'source': 'source'
    }
    
    # Renommer les colonnes
    data = data.rename(columns=column_mapping)
    
    # S'assurer que published_at est datetime
    if 'published_at' in data.columns:
        data['published_at'] = pd.to_datetime(data['published_at'])
    
    # Ajouter ticker si manquant
    if 'ticker' not in data.columns:
        data['ticker'] = 'UNKNOWN'
    
    return data

def normalize_features_data(data):
    """Normalise les donn√©es de features"""
    # V√©rifier que les features requises sont pr√©sentes
    required_features = CONSTANTS.get_feature_columns()
    missing_features = [f for f in required_features if f not in data.columns]
    
    if missing_features:
        print(f"  ‚ö†Ô∏è Features manquantes: {missing_features}")
    
    # Ajouter ticker si manquant
    if 'ticker' not in data.columns:
        data['ticker'] = 'UNKNOWN'
    
    return data

def extract_ticker_from_filename(filename):
    """Extrait le ticker depuis le nom de fichier"""
    filename_lower = filename.lower()
    
    if 'spy' in filename_lower:
        return 'SPY'
    elif 'nvda' in filename_lower:
        return 'NVDA'
    else:
        return 'UNKNOWN'

def cleanup_old_directories():
    """Nettoie les anciens r√©pertoires apr√®s migration"""
    print("üßπ Nettoyage des anciens r√©pertoires...")
    
    # R√©pertoires √† nettoyer (garder une copie de sauvegarde)
    old_dirs = [
        "data/dataset",
        "data/trading",
        "data/backup"
    ]
    
    for old_dir in old_dirs:
        if Path(old_dir).exists():
            # Cr√©er une sauvegarde
            backup_dir = f"data/backup_migrated_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
            shutil.move(old_dir, backup_dir)
            print(f"  ‚úÖ {old_dir} d√©plac√© vers {backup_dir}")

def verify_migration():
    """V√©rifie que la migration s'est bien pass√©e"""
    print("üîç V√©rification de la migration...")
    
    storage = UnifiedDataStorage()
    summary = storage.get_data_summary()
    
    print("\nüìä R√©sum√© des donn√©es migr√©es:")
    print(f"  Prix: {len(summary['prices'])} tickers")
    print(f"  News: {summary['news'].get('files', 0)} fichiers")
    print(f"  Features: {len(summary['features'])} tickers")
    print(f"  Mod√®les: {len(summary['models'])} tickers")
    
    # V√©rifier que les donn√©es essentielles sont pr√©sentes
    if summary['prices']:
        print("  ‚úÖ Donn√©es de prix migr√©es")
    else:
        print("  ‚ùå Aucune donn√©e de prix trouv√©e")
    
    if summary['features']:
        print("  ‚úÖ Donn√©es de features migr√©es")
    else:
        print("  ‚ùå Aucune donn√©e de features trouv√©e")
    
    if summary['models']:
        print("  ‚úÖ Mod√®les migr√©s")
    else:
        print("  ‚ùå Aucun mod√®le trouv√©")

if __name__ == "__main__":
    print("üöÄ Migration vers stockage unifi√© Sentinel2")
    print("=" * 50)
    
    try:
        migrate_existing_data()
        verify_migration()
        print("\nüéâ Migration termin√©e avec succ√®s!")
        
    except Exception as e:
        print(f"\n‚ùå Erreur lors de la migration: {e}")
        sys.exit(1)
