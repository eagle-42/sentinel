#!/usr/bin/env python3
"""
ğŸ•·ï¸ Yahoo Finance Scraper
Scrape les donnÃ©es de prix directement depuis Yahoo Finance
Contourne les limitations des APIs gratuites
"""

import sys
from pathlib import Path
from datetime import datetime, timedelta, timezone
from typing import Optional, List, Dict
import time
import random

import pandas as pd
import requests
from bs4 import BeautifulSoup
from loguru import logger

# Ajouter src au path
sys.path.insert(0, str(Path(__file__).parent.parent / "src"))

from constants import CONSTANTS


class YahooFinanceScraper:
    """Scraper pour Yahoo Finance"""
    
    def __init__(self):
        self.base_url = "https://finance.yahoo.com"
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.5',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Sec-Fetch-Dest': 'document',
            'Sec-Fetch-Mode': 'navigate',
            'Sec-Fetch-Site': 'none',
            'Cache-Control': 'max-age=0'
        })
    
    def get_current_price(self, ticker: str) -> Optional[float]:
        """RÃ©cupÃ¨re le prix actuel via yfinance (plus fiable que scraping)"""
        try:
            import yfinance as yf
            
            logger.info(f"ğŸ“Š RÃ©cupÃ©ration prix actuel pour {ticker} via yfinance")
            
            # Utiliser l'API fast_info de yfinance qui est plus stable
            stock = yf.Ticker(ticker)
            
            # Essayer fast_info d'abord
            try:
                price = stock.fast_info['lastPrice']
                if price and price > 0:
                    logger.info(f"âœ… Prix actuel {ticker}: ${price:.2f}")
                    return float(price)
            except:
                pass
            
            # Fallback: info dict
            try:
                info = stock.info
                price = info.get('regularMarketPrice') or info.get('currentPrice') or info.get('previousClose')
                if price and price > 0:
                    logger.info(f"âœ… Prix actuel {ticker} (info): ${price:.2f}")
                    return float(price)
            except:
                pass
            
            # Dernier fallback: history 1 jour
            try:
                hist = stock.history(period='1d', interval='1m')
                if not hist.empty:
                    price = hist['Close'].iloc[-1]
                    logger.info(f"âœ… Prix actuel {ticker} (history): ${price:.2f}")
                    return float(price)
            except:
                pass
            
            logger.warning(f"âš ï¸ Impossible de rÃ©cupÃ©rer le prix pour {ticker}")
            return None
            
        except Exception as e:
            logger.error(f"âŒ Erreur rÃ©cupÃ©ration prix {ticker}: {e}")
            return None
    
    def get_historical_data(self, ticker: str, period: str = "1d", interval: str = "15m") -> pd.DataFrame:
        """
        RÃ©cupÃ¨re les donnÃ©es historiques via l'API JSON Yahoo cachÃ©e
        
        Args:
            ticker: Symbole du ticker (ex: SPY)
            period: PÃ©riode (1d, 5d, 1mo, 3mo, 6mo, 1y, 2y, 5y, 10y, ytd, max)
            interval: Intervalle (1m, 2m, 5m, 15m, 30m, 60m, 90m, 1h, 1d, 5d, 1wk, 1mo, 3mo)
        """
        try:
            # Convertir period en timestamps Unix
            end_time = int(datetime.now().timestamp())
            
            period_map = {
                '1d': 1, '5d': 5, '1mo': 30, '3mo': 90,
                '6mo': 180, '1y': 365, '2y': 730, '5y': 1825
            }
            days = period_map.get(period, 5)  # Par dÃ©faut 5 jours
            start_time = int((datetime.now() - timedelta(days=days)).timestamp())
            
            # Utiliser l'API JSON cachÃ©e de Yahoo Finance v8
            url = f"https://query1.finance.yahoo.com/v8/finance/chart/{ticker}"
            params = {
                'period1': start_time,
                'period2': end_time,
                'interval': interval,
                'includePrePost': 'false',
                'events': 'div,splits'
            }
            
            logger.info(f"ğŸ“Š RequÃªte API Yahoo pour {ticker} ({period}, {interval})")
            
            response = self.session.get(url, params=params, timeout=15)
            response.raise_for_status()
            
            data_json = response.json()
            
            # VÃ©rifier si erreur
            if 'chart' not in data_json:
                logger.warning(f"âš ï¸ Format JSON invalide pour {ticker}")
                return pd.DataFrame()
            
            chart = data_json['chart']
            
            if 'error' in chart and chart['error']:
                logger.error(f"âŒ Erreur API: {chart['error']}")
                return pd.DataFrame()
            
            if not chart.get('result'):
                logger.warning(f"âš ï¸ Aucun rÃ©sultat pour {ticker}")
                return pd.DataFrame()
            
            result = chart['result'][0]
            
            # Extraire les donnÃ©es
            timestamps = result['timestamp']
            quotes = result['indicators']['quote'][0]
            
            data = []
            for i, ts in enumerate(timestamps):
                # VÃ©rifier que les valeurs ne sont pas None
                if (quotes['close'][i] is not None and 
                    quotes['open'][i] is not None):
                    
                    data.append({
                        'ts_utc': datetime.fromtimestamp(ts, tz=timezone.utc),
                        'open': float(quotes['open'][i]),
                        'high': float(quotes['high'][i]),
                        'low': float(quotes['low'][i]),
                        'close': float(quotes['close'][i]),
                        'volume': int(quotes['volume'][i]) if quotes['volume'][i] else 0
                    })
            
            if not data:
                logger.warning(f"âš ï¸ Aucune donnÃ©e valide pour {ticker}")
                return pd.DataFrame()
            
            df = pd.DataFrame(data)
            df['ticker'] = ticker
            
            logger.info(f"âœ… {len(df)} barres rÃ©cupÃ©rÃ©es pour {ticker}")
            logger.info(f"ğŸ“… PÃ©riode: {df['ts_utc'].min()} Ã  {df['ts_utc'].max()}")
            
            return df
            
        except Exception as e:
            logger.error(f"âŒ Erreur API Yahoo {ticker}: {e}")
            import traceback
            traceback.print_exc()
            return pd.DataFrame()
    
    def scrape_with_retry(self, ticker: str, period: str = "1d", interval: str = "15m", 
                         max_retries: int = 3) -> pd.DataFrame:
        """Scrape avec retry et backoff"""
        for attempt in range(max_retries):
            try:
                if attempt > 0:
                    delay = 2 ** attempt + random.uniform(0, 1)
                    logger.info(f"ğŸ”„ Tentative {attempt + 1}/{max_retries} (attente {delay:.1f}s)")
                    time.sleep(delay)
                
                df = self.get_historical_data(ticker, period, interval)
                
                if not df.empty:
                    return df
                
            except Exception as e:
                logger.warning(f"âš ï¸ Tentative {attempt + 1} Ã©chouÃ©e: {e}")
                if attempt == max_retries - 1:
                    logger.error(f"âŒ Ã‰chec dÃ©finitif aprÃ¨s {max_retries} tentatives")
        
        return pd.DataFrame()


def refresh_prices_scraper(ticker: str = "SPY") -> bool:
    """
    Scrape UNIQUEMENT le prix actuel et l'ajoute Ã  l'historique
    Approche simple : on construit notre propre historique 15min
    """
    try:
        logger.info(f"ğŸ•·ï¸ Scraping prix actuel pour {ticker}")
        
        scraper = YahooFinanceScraper()
        
        # RÃ©cupÃ©rer le prix actuel
        current_price = scraper.get_current_price(ticker)
        
        if current_price is None:
            logger.error(f"âŒ Impossible de rÃ©cupÃ©rer le prix pour {ticker}")
            return False
        
        # CrÃ©er une barre pour maintenant
        now = datetime.now(timezone.utc)
        # Arrondir Ã  la fenÃªtre 15min la plus proche
        minutes = (now.minute // 15) * 15
        ts_rounded = now.replace(minute=minutes, second=0, microsecond=0)
        
        new_bar = pd.DataFrame([{
            'ts_utc': ts_rounded,
            'open': current_price,
            'high': current_price,
            'low': current_price,
            'close': current_price,
            'volume': 0,  # On n'a pas le volume en scrapant
            'ticker': ticker
        }])
        
        logger.info(f"ğŸ“Š Nouvelle barre: {ts_rounded} | ${current_price:.2f}")
        
        # Charger les donnÃ©es existantes
        file_path = Path(f"data/realtime/prices/{ticker.lower()}_15min.parquet")
        
        if file_path.exists():
            existing_data = pd.read_parquet(file_path)
            existing_data['ts_utc'] = pd.to_datetime(existing_data['ts_utc'])
            logger.info(f"ğŸ“Š DonnÃ©es existantes: {len(existing_data)} lignes")
            
            # VÃ©rifier si on a dÃ©jÃ  cette timestamp
            if ts_rounded in existing_data['ts_utc'].values:
                # Mettre Ã  jour la barre existante (close, high, low)
                idx = existing_data[existing_data['ts_utc'] == ts_rounded].index[0]
                existing_data.loc[idx, 'close'] = current_price
                existing_data.loc[idx, 'high'] = max(existing_data.loc[idx, 'high'], current_price)
                existing_data.loc[idx, 'low'] = min(existing_data.loc[idx, 'low'], current_price)
                combined = existing_data
                logger.info(f"ğŸ”„ Barre {ts_rounded} mise Ã  jour")
            else:
                # Ajouter la nouvelle barre
                combined = pd.concat([existing_data, new_bar], ignore_index=True)
                combined = combined.sort_values('ts_utc').reset_index(drop=True)
                logger.info(f"â• Nouvelle barre ajoutÃ©e")
        else:
            combined = new_bar
            logger.info(f"ğŸ’¾ Premier enregistrement pour {ticker}")
        
        # Sauvegarder
        file_path.parent.mkdir(parents=True, exist_ok=True)
        combined.to_parquet(file_path, index=False)
        
        logger.info(f"âœ… DonnÃ©es sauvegardÃ©es: {file_path}")
        logger.info(f"ğŸ“… Total: {len(combined)} barres de {combined['ts_utc'].min()} Ã  {combined['ts_utc'].max()}")
        
        return True
        
    except Exception as e:
        logger.error(f"âŒ Erreur refresh scraper: {e}")
        import traceback
        traceback.print_exc()
        return False


def main():
    """Point d'entrÃ©e du script"""
    logger.info("ğŸš€ Yahoo Finance Scraper")
    logger.info("=" * 60)
    
    success = refresh_prices_scraper("SPY")
    
    if success:
        logger.info("\nâœ… Scraping terminÃ© avec succÃ¨s")
    else:
        logger.error("\nâŒ Ã‰chec du scraping")
        sys.exit(1)


if __name__ == "__main__":
    main()
