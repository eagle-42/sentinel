#!/usr/bin/env python3
"""
üîÑ Script de mise √† jour des donn√©es de prix
Met √† jour les donn√©es de prix toutes les 15 minutes depuis Yahoo Finance et Polygon API
"""

import os
import sys
from datetime import datetime, timedelta, timezone
from pathlib import Path
from typing import Any, Dict, List, Optional

import pandas as pd
import requests
import yfinance as yf
from dotenv import load_dotenv
from loguru import logger

# Charger les variables d'environnement
load_dotenv()

# Ajouter src au path
sys.path.insert(0, str(Path(__file__).parent.parent / "src"))

from src.config import config
from src.constants import CONSTANTS
from src.data.storage import DataStorage


class PriceRefresher:
    """Gestionnaire de mise √† jour des donn√©es de prix"""

    def __init__(self):
        self.storage = DataStorage()
        self.tickers = CONSTANTS.TICKERS
        self.polygon_api_key = os.getenv("POLYGON_API_KEY")
        self.yahoo_fallback = True

    def get_yahoo_prices(self, ticker: str, period: str = "7d", interval: str = "15m") -> pd.DataFrame:
        """R√©cup√®re les donn√©es de prix depuis Yahoo Finance avec retry et backoff"""
        import random
        import time

        max_retries = 3
        base_delay = 5  # secondes

        for attempt in range(max_retries):
            try:
                if attempt > 0:
                    delay = base_delay * (2**attempt) + random.uniform(0, 1)
                    logger.info(f"üîÑ Tentative {attempt + 1}/{max_retries} pour {ticker} (attente {delay:.1f}s)")
                    time.sleep(delay)

                logger.info(f"üìà R√©cup√©ration {ticker} depuis Yahoo Finance ({period}, {interval})")

                ticker_obj = yf.Ticker(ticker)
                data = ticker_obj.history(period=period, interval=interval)

                if data.empty:
                    logger.warning(f"‚ö†Ô∏è Aucune donn√©e Yahoo pour {ticker} (tentative {attempt + 1})")
                    continue

                # Normaliser les donn√©es
                data = data.reset_index()
                data["ticker"] = ticker
                data = data.rename(columns={"Datetime": "ts_utc"})

                # S'assurer que la date est en UTC
                if data["ts_utc"].dt.tz is None:
                    data["ts_utc"] = data["ts_utc"].dt.tz_localize("UTC")
                else:
                    data["ts_utc"] = data["ts_utc"].dt.tz_convert("UTC")

                # S√©lectionner les colonnes finales
                data = data[["ticker", "ts_utc", "Open", "High", "Low", "Close", "Volume"]]
                data.columns = ["ticker", "ts_utc", "open", "high", "low", "close", "volume"]

                logger.info(f"‚úÖ {ticker}: {len(data)} barres r√©cup√©r√©es depuis Yahoo")
                return data

            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Erreur Yahoo pour {ticker} (tentative {attempt + 1}): {e}")
                if attempt == max_retries - 1:
                    logger.error(f"‚ùå √âchec d√©finitif Yahoo pour {ticker} apr√®s {max_retries} tentatives")
                    return pd.DataFrame()
                continue

        return pd.DataFrame()

    def get_polygon_prices(self, ticker: str, start_date: datetime, end_date: datetime) -> pd.DataFrame:
        """R√©cup√®re les donn√©es de prix depuis Polygon API"""
        if not self.polygon_api_key:
            logger.warning("‚ö†Ô∏è Cl√© API Polygon manquante, utilisation de Yahoo Finance")
            return pd.DataFrame()

        try:
            logger.info(f"üìà R√©cup√©ration {ticker} depuis Polygon API")

            # Convertir les dates en timestamps Unix
            start_ts = int(start_date.timestamp() * 1000)
            end_ts = int(end_date.timestamp() * 1000)

            url = f"https://api.polygon.io/v2/aggs/ticker/{ticker}/range/15/minute/{start_ts}/{end_ts}"
            params = {"adjusted": "true", "sort": "asc", "apikey": self.polygon_api_key}

            response = requests.get(url, params=params, timeout=30)
            response.raise_for_status()

            data = response.json()

            if data.get("status") not in ["OK", "DELAYED"] or not data.get("results"):
                logger.warning(f"‚ö†Ô∏è Aucune donn√©e Polygon pour {ticker} (status: {data.get('status')})")
                return pd.DataFrame()

            # Convertir les donn√©es
            results = data["results"]
            df_data = []

            for bar in results:
                df_data.append(
                    {
                        "ticker": ticker,
                        "ts_utc": pd.to_datetime(bar["t"], unit="ms", utc=True),
                        "open": bar["o"],
                        "high": bar["h"],
                        "low": bar["l"],
                        "close": bar["c"],
                        "volume": bar["v"],
                    }
                )

            df = pd.DataFrame(df_data)
            logger.info(f"‚úÖ {ticker}: {len(df)} barres r√©cup√©r√©es depuis Polygon")
            return df

        except Exception as e:
            logger.error(f"‚ùå Erreur Polygon pour {ticker}: {e}")
            return pd.DataFrame()

    def should_refresh_data(self, ticker: str, interval: str = "15min") -> bool:
        """V√©rifie si les donn√©es doivent √™tre rafra√Æchies"""
        try:
            # V√©rifier si le fichier existe
            file_path = CONSTANTS.get_data_path("prices", ticker, interval)
            if not file_path.exists():
                logger.info(f"üÜï Nouveau fichier pour {ticker}")
                return True

            # Lire la derni√®re date
            existing_data = pd.read_parquet(file_path)
            if existing_data.empty:
                logger.info(f"üìÅ Fichier vide pour {ticker}")
                return True

            last_update = existing_data["ts_utc"].max()
            if hasattr(last_update, "to_pydatetime"):
                last_update = last_update.to_pydatetime()
            if last_update.tzinfo is None:
                last_update = last_update.replace(tzinfo=timezone.utc)

            # V√©rifier si les donn√©es ont plus de 1 heure
            time_diff = (datetime.now(timezone.utc) - last_update).total_seconds()
            should_refresh = time_diff > 3600  # 1 heure

            if should_refresh:
                logger.info(f"üîÑ {ticker}: Donn√©es anciennes ({time_diff/3600:.1f}h), refresh n√©cessaire")
            else:
                logger.info(f"‚úÖ {ticker}: Donn√©es r√©centes ({time_diff/60:.1f}min), pas de refresh")

            return should_refresh

        except Exception as e:
            logger.error(f"‚ùå Erreur v√©rification refresh {ticker}: {e}")
            return True

    def merge_price_data(self, existing_data: pd.DataFrame, new_data: pd.DataFrame) -> pd.DataFrame:
        """Fusionne les donn√©es existantes et nouvelles"""
        if existing_data.empty:
            return new_data

        if new_data.empty:
            return existing_data

        # Concat√©ner et supprimer les doublons
        combined = pd.concat([existing_data, new_data], ignore_index=True)
        combined = combined.drop_duplicates(subset=["ts_utc"], keep="last")
        combined = combined.sort_values("ts_utc").reset_index(drop=True)

        return combined

    def refresh_ticker_data(self, ticker: str) -> Dict[str, Any]:
        """Met √† jour les donn√©es pour un ticker sp√©cifique"""
        logger.info(f"\nüìà Traitement {ticker}")

        # V√©rifier si refresh n√©cessaire
        if not self.should_refresh_data(ticker):
            return {"ticker": ticker, "status": "up_to_date", "rows": 0, "new_rows": 0, "source": "none"}

        # D√©terminer la p√©riode (7 derniers jours)
        end_date = datetime.now(timezone.utc)
        start_date = end_date - timedelta(days=7)

        # Essayer Polygon d'abord, puis Yahoo en fallback
        new_data = self.get_polygon_prices(ticker, start_date, end_date)
        source = "polygon"

        if new_data.empty and self.yahoo_fallback:
            logger.info(f"üîÑ Fallback vers Yahoo Finance pour {ticker}")
            new_data = self.get_yahoo_prices(ticker, period="7d", interval="15m")
            source = "yahoo"

        if new_data.empty:
            logger.warning(f"‚ùå Aucune donn√©e r√©cup√©r√©e pour {ticker}")
            return {"ticker": ticker, "status": "no_data", "rows": 0, "new_rows": 0, "source": "none"}

        # Charger les donn√©es existantes
        file_path = CONSTANTS.get_data_path("prices", ticker, "15min")
        existing_data = pd.DataFrame()

        if file_path.exists():
            try:
                existing_data = pd.read_parquet(file_path)
                logger.info(f"üìä {ticker}: {len(existing_data)} lignes existantes")
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Erreur lecture donn√©es existantes {ticker}: {e}")

        # Fusionner les donn√©es
        combined_data = self.merge_price_data(existing_data, new_data)

        # Sauvegarder
        try:
            combined_data.to_parquet(file_path, index=False)
            logger.info(f"üíæ {ticker}: {len(combined_data)} lignes sauvegard√©es")
            logger.info(f"üìÖ P√©riode: {combined_data['ts_utc'].min()} √† {combined_data['ts_utc'].max()}")

            return {
                "ticker": ticker,
                "status": "updated",
                "rows": len(combined_data),
                "new_rows": len(new_data),
                "existing_rows": len(existing_data),
                "source": source,
                "start_date": combined_data["ts_utc"].min().isoformat(),
                "end_date": combined_data["ts_utc"].max().isoformat(),
            }

        except Exception as e:
            logger.error(f"‚ùå Erreur sauvegarde {ticker}: {e}")
            return {"ticker": ticker, "status": "error", "rows": 0, "new_rows": 0, "source": source, "error": str(e)}

    def refresh_all_prices(self) -> Dict[str, Any]:
        """Met √† jour toutes les donn√©es de prix"""
        logger.info("üîÑ === MISE √Ä JOUR DES DONN√âES DE PRIX ===")
        start_time = datetime.now()

        results = {}
        total_rows = 0
        total_new_rows = 0

        for ticker in self.tickers:
            result = self.refresh_ticker_data(ticker)
            results[ticker] = result

            if result["status"] == "updated":
                total_rows += result["rows"]
                total_new_rows += result["new_rows"]

        # Sauvegarder l'√©tat
        state = {
            "last_update": datetime.now().isoformat(),
            "granularity": "15min",
            "period": f"7 derniers jours",
            "tickers": results,
            "summary": {
                "total_tickers": len(self.tickers),
                "total_rows": total_rows,
                "total_new_rows": total_new_rows,
                "duration_seconds": (datetime.now() - start_time).total_seconds(),
            },
        }

        state_path = CONSTANTS.DATA_ROOT / "logs" / "price_refresh_state.json"
        state_path.parent.mkdir(parents=True, exist_ok=True)

        import json

        with open(state_path, "w") as f:
            json.dump(state, f, indent=2)

        logger.info(f"\nüìä R√©sum√© de la mise √† jour:")
        logger.info(f"   Tickers trait√©s: {len(self.tickers)}")
        logger.info(f"   Lignes totales: {total_rows}")
        logger.info(f"   Nouvelles lignes: {total_new_rows}")
        logger.info(f"   Dur√©e: {(datetime.now() - start_time).total_seconds():.1f}s")
        logger.info(f"   √âtat sauv√©: {state_path}")

        return state


def main():
    """Fonction principale"""
    logger.info("üöÄ D√©marrage du refresh des donn√©es de prix")

    try:
        refresher = PriceRefresher()
        state = refresher.refresh_all_prices()

        logger.info("‚úÖ Refresh des donn√©es de prix termin√© avec succ√®s")
        return 0

    except Exception as e:
        logger.error(f"‚ùå Erreur lors du refresh des donn√©es: {e}")
        return 1


if __name__ == "__main__":
    exit_code = main()
    sys.exit(exit_code)
